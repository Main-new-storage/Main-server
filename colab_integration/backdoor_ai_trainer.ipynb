{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backdoor AI Model Trainer\n",
    "\n",
    "This notebook trains Backdoor AI models using Google Colab's resources. It connects to Dropbox to access training data and upload trained models.\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages with specific versions for compatibility\n",
    "!pip install dropbox pandas numpy nltk flask\n",
    "# Install specific scikit-learn version compatible with coremltools\n",
    "!pip install scikit-learn==1.5.1\n",
    "# Install joblib\n",
    "!pip install joblib\n",
    "# Install coremltools with specific flags to ensure proper installation\n",
    "!pip uninstall -y coremltools\n",
    "!pip install coremltools==6.3 --no-binary coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('backdoor_ai_trainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropbox Configuration - Edit these settings if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dropbox credentials\n",
    "DROPBOX_APP_KEY = \"2bi422xpd3xd962\"  # Default from your config\n",
    "DROPBOX_APP_SECRET = \"j3yx0b41qdvfu86\"  # Default from your config\n",
    "DROPBOX_REFRESH_TOKEN = \"RvyL03RE5qAAAAAAAAAAAVMVebvE7jDx8Okd0ploMzr85c6txvCRXpJAt30mxrKF\"  # Default from your config\n",
    "\n",
    "# Configuration for folders and files\n",
    "DROPBOX_DB_FILENAME = \"backdoor_ai_db.db\"\n",
    "DROPBOX_MODELS_FOLDER = \"backdoor_models\"\n",
    "DROPBOX_BASE_MODEL_FOLDER = \"base_model\"\n",
    "DROPBOX_TRIGGERS_FOLDER = \"training_triggers\"\n",
    "\n",
    "# Training configuration\n",
    "MODEL_VERSION_PREFIX = \"1.0.\"\n",
    "MAX_FEATURES = 5000\n",
    "NGRAM_RANGE = (1, 2)\n",
    "BASE_MODEL_WEIGHT = 2.0\n",
    "USER_MODEL_WEIGHT = 1.0\n",
    "MIN_TRAINING_DATA = 50\n",
    "MAX_MODELS_TO_KEEP = 5\n",
    "\n",
    "# Retraining thresholds\n",
    "RETRAINING_THRESHOLDS = {\n",
    "    'pending_models': 3,\n",
    "    'hours_since_last_training': 12,\n",
    "    'new_interactions': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Dropbox connection\n",
    "def init_dropbox():\n",
    "    import dropbox\n",
    "    import requests\n",
    "    \n",
    "    # Get access token using refresh token\n",
    "    token_url = \"https://api.dropboxapi.com/oauth2/token\"\n",
    "    data = {\n",
    "        \"grant_type\": \"refresh_token\",\n",
    "        \"refresh_token\": DROPBOX_REFRESH_TOKEN,\n",
    "        \"client_id\": DROPBOX_APP_KEY,\n",
    "        \"client_secret\": DROPBOX_APP_SECRET\n",
    "    }\n",
    "    \n",
    "    response = requests.post(token_url, data=data)\n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        access_token = token_data[\"access_token\"]\n",
    "        print(\"Successfully obtained access token\")\n",
    "        \n",
    "        # Initialize Dropbox client\n",
    "        dbx = dropbox.Dropbox(\n",
    "            oauth2_access_token=access_token,\n",
    "            app_key=DROPBOX_APP_KEY,\n",
    "            app_secret=DROPBOX_APP_SECRET\n",
    "        )\n",
    "        \n",
    "        # Test connection\n",
    "        account = dbx.users_get_current_account()\n",
    "        print(f\"Connected to Dropbox as {account.name.display_name}\")\n",
    "        return dbx\n",
    "    else:\n",
    "        print(f\"Token refresh failed: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Connect to Dropbox\n",
    "dbx = init_dropbox()\n",
    "if dbx is None:\n",
    "    raise Exception(\"Failed to connect to Dropbox. Check your credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropbox File Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dropbox utility functions\n",
    "from dropbox.files import WriteMode\n",
    "from dropbox.exceptions import ApiError\n",
    "\n",
    "def ensure_folder_exists(path):\n",
    "    try:\n",
    "        dbx.files_get_metadata(path)\n",
    "        return True\n",
    "    except ApiError as e:\n",
    "        if e.error.is_path() and e.error.get_path().is_not_found():\n",
    "            dbx.files_create_folder_v2(path)\n",
    "            print(f\"Created folder: {path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error checking folder: {e}\")\n",
    "            return False\n",
    "\n",
    "def download_file(path, local_path=None):\n",
    "    try:\n",
    "        if local_path:\n",
    "            dbx.files_download_to_file(local_path, path)\n",
    "            return {'success': True, 'path': local_path}\n",
    "        else:\n",
    "            metadata, response = dbx.files_download(path)\n",
    "            content = response.content\n",
    "            buffer = io.BytesIO(content)\n",
    "            return {'success': True, 'buffer': buffer, 'size': len(content)}\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file {path}: {e}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "def upload_file(data_or_path, path):\n",
    "    try:\n",
    "        if isinstance(data_or_path, str) and os.path.exists(data_or_path):\n",
    "            with open(data_or_path, 'rb') as f:\n",
    "                dbx.files_upload(f.read(), path, mode=WriteMode.overwrite)\n",
    "        elif hasattr(data_or_path, 'read'):\n",
    "            if hasattr(data_or_path, 'seek'):\n",
    "                data_or_path.seek(0)\n",
    "            dbx.files_upload(data_or_path.read(), path, mode=WriteMode.overwrite)\n",
    "        else:\n",
    "            dbx.files_upload(data_or_path, path, mode=WriteMode.overwrite)\n",
    "        return {'success': True, 'path': path}\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to {path}: {e}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "def list_folder(path):\n",
    "    try:\n",
    "        result = dbx.files_list_folder(path)\n",
    "        items = result.entries\n",
    "        while result.has_more:\n",
    "            result = dbx.files_list_folder_continue(result.cursor)\n",
    "            items.extend(result.entries)\n",
    "        return {'success': True, 'items': items}\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing folder {path}: {e}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "# Create required folders\n",
    "for folder in [\n",
    "    f\"/{DROPBOX_MODELS_FOLDER}\", \n",
    "    f\"/{DROPBOX_BASE_MODEL_FOLDER}\", \n",
    "    f\"/{DROPBOX_TRIGGERS_FOLDER}\",\n",
    "    f\"/{DROPBOX_MODELS_FOLDER}/trained\",\n",
    "    f\"/{DROPBOX_MODELS_FOLDER}/uploaded\"\n",
    "]:\n",
    "    ensure_folder_exists(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get the database from Dropbox\n",
    "def setup_database():\n",
    "    db_path = f\"/{DROPBOX_DB_FILENAME}\"\n",
    "    temp_db_path = os.path.join(tempfile.gettempdir(), DROPBOX_DB_FILENAME)\n",
    "    \n",
    "    # Download database file\n",
    "    result = download_file(db_path, temp_db_path)\n",
    "    if not result['success']:\n",
    "        print(f\"Failed to download database: {result.get('error')}\")\n",
    "        return None\n",
    "    \n",
    "    # Create in-memory database for faster operations\n",
    "    conn_memory = sqlite3.connect(':memory:')\n",
    "    conn_disk = sqlite3.connect(temp_db_path)\n",
    "    conn_disk.backup(conn_memory)\n",
    "    conn_disk.close()\n",
    "    \n",
    "    print(f\"Database loaded into memory from {temp_db_path}\")\n",
    "    return conn_memory\n",
    "\n",
    "# Connect to database\n",
    "db_conn = setup_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Training Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Determine if training is needed\n",
    "def should_retrain(conn):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check pending models\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM uploaded_models WHERE incorporation_status = 'pending'\")\n",
    "    pending_models_count = cursor.fetchone()[0]\n",
    "    \n",
    "    if pending_models_count >= RETRAINING_THRESHOLDS['pending_models']:\n",
    "        print(f\"Retraining triggered: {pending_models_count} pending uploaded models\")\n",
    "        return True\n",
    "    \n",
    "    # Check time since last training\n",
    "    cursor.execute(\"SELECT MAX(training_date) FROM model_versions\")\n",
    "    last_training = cursor.fetchone()[0]\n",
    "    \n",
    "    if last_training:\n",
    "        last_training_date = datetime.fromisoformat(last_training)\n",
    "        time_since_training = datetime.now() - last_training_date\n",
    "        \n",
    "        if (time_since_training > timedelta(hours=RETRAINING_THRESHOLDS['hours_since_last_training']) \n",
    "            and pending_models_count > 0):\n",
    "            print(f\"Retraining triggered by time threshold\")\n",
    "            return True\n",
    "        \n",
    "        # Check new interactions\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM interactions WHERE created_at > ?\", (last_training,))\n",
    "        new_interactions = cursor.fetchone()[0]\n",
    "        \n",
    "        if (new_interactions >= RETRAINING_THRESHOLDS['new_interactions'] \n",
    "            and pending_models_count > 0):\n",
    "            print(f\"Retraining triggered by new interactions\")\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Get pending models\n",
    "def get_pending_uploaded_models(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT id, device_id, file_path, upload_date, app_version, description, file_size, original_filename \n",
    "        FROM uploaded_models\n",
    "        WHERE incorporation_status IN ('pending', 'processing')\n",
    "        ORDER BY upload_date ASC\n",
    "    \"\"\")\n",
    "    \n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    models = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "    \n",
    "    # Download model files from Dropbox\n",
    "    for model in models:\n",
    "        if model['file_path'].startswith('dropbox:'):\n",
    "            path = model['file_path'].split(':', 1)[1]\n",
    "            try:\n",
    "                result = download_file(path)\n",
    "                if result['success']:\n",
    "                    model['model_buffer'] = result['buffer']\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model {model['id']}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Check for training triggers\n",
    "def check_for_triggers():\n",
    "    triggers_path = f\"/{DROPBOX_TRIGGERS_FOLDER}\"\n",
    "    result = list_folder(triggers_path)\n",
    "    \n",
    "    if not result['success']:\n",
    "        return None\n",
    "    \n",
    "    for item in result['items']:\n",
    "        if hasattr(item, 'name') and item.name.endswith('training_needed.json'):\n",
    "            trigger_path = item.path_display\n",
    "            trigger_result = download_file(trigger_path)\n",
    "            \n",
    "            if trigger_result['success']:\n",
    "                buffer = trigger_result['buffer']\n",
    "                buffer.seek(0)\n",
    "                try:\n",
    "                    trigger_data = json.loads(buffer.read().decode('utf-8'))\n",
    "                    print(f\"Found trigger file: {trigger_path}\")\n",
    "                    return {'path': trigger_path, 'data': trigger_data}\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing trigger file: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Update trigger status\n",
    "def update_trigger_status(trigger_path, status, message=None, model_info=None):\n",
    "    result = download_file(trigger_path)\n",
    "    if not result['success']:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        buffer = result['buffer']\n",
    "        buffer.seek(0)\n",
    "        trigger_data = json.loads(buffer.read().decode('utf-8'))\n",
    "        \n",
    "        trigger_data['status'] = status\n",
    "        trigger_data['updated_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        if message:\n",
    "            trigger_data['message'] = message\n",
    "        if model_info:\n",
    "            trigger_data['model_info'] = model_info\n",
    "        \n",
    "        updated_buffer = io.BytesIO(json.dumps(trigger_data).encode('utf-8'))\n",
    "        return upload_file(updated_buffer, trigger_path)['success']\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating trigger: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check if training is needed\n",
    "if db_conn:\n",
    "    needs_training = should_retrain(db_conn)\n",
    "    pending_models = get_pending_uploaded_models(db_conn)\n",
    "    print(f\"Training needed: {needs_training}\")\n",
    "    print(f\"Found {len(pending_models)} pending models\")\n",
    "    \n",
    "    # Check for trigger files\n",
    "    trigger = check_for_triggers()\n",
    "    if trigger:\n",
    "        print(\"Found training trigger file\")\n",
    "        update_trigger_status(trigger['path'], 'processing', \"Training started in Google Colab\")\n",
    "        force_training = True\n",
    "    else:\n",
    "        force_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import ML libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import sys\n",
    "import coremltools as ct\n",
    "\n",
    "# Download NLTK resources\n",
    "for resource in ['punkt', 'stopwords', 'wordnet']:\n",
    "    nltk.download(resource, quiet=True)\n",
    "\n",
    "# Text preprocessing functions\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase and tokenize\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Get stopwords\n",
    "    stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n",
    "    \n",
    "    # Process tokens\n",
    "    processed_tokens = []\n",
    "    lemmatizer = WordNetLemmatizer() if lemmatize else None\n",
    "    \n",
    "    for token in tokens:\n",
    "        if not token.isalnum() or token in stop_words:\n",
    "            continue\n",
    "        if lemmatize and lemmatizer:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "        processed_tokens.append(token)\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Define IntentClassifier class\n",
    "class IntentClassifier:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        self.classes = None\n",
    "        self.is_ensemble = False\n",
    "        self.component_models = {}\n",
    "        self.model_version = None\n",
    "        self.training_date = None\n",
    "        self.accuracy = None\n",
    "        self.training_data_size = 0\n",
    "    \n",
    "    @property\n",
    "    def is_trained(self):\n",
    "        return self.model is not None and self.vectorizer is not None\n",
    "    \n",
    "    def train(self, data, user_message_col='user_message', intent_col='detected_intent',\n",
    "             weight_col=None, test_size=0.2):\n",
    "        # Preprocess text\n",
    "        data['processed_message'] = data[user_message_col].apply(preprocess_text)\n",
    "        \n",
    "        # Split data\n",
    "        X = data['processed_message']\n",
    "        y = data[intent_col]\n",
    "        weights = data[weight_col].values if weight_col in data.columns else None\n",
    "        \n",
    "        X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "            X, y, weights, test_size=test_size, random_state=42, \n",
    "            stratify=y if len(set(y)) > 1 else None\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        self.vectorizer = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=NGRAM_RANGE)\n",
    "        X_train_vec = self.vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = self.vectorizer.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        self.model.fit(X_train_vec, y_train, sample_weight=w_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.model.predict(X_test_vec)\n",
    "        self.accuracy = accuracy_score(y_test, y_pred, sample_weight=w_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        # Set metadata\n",
    "        self.classes = self.model.classes_.tolist()\n",
    "        self.training_data_size = len(X_train)\n",
    "        self.training_date = datetime.now().isoformat()\n",
    "        timestamp = int(datetime.now().timestamp())\n",
    "        self.model_version = f\"{MODEL_VERSION_PREFIX}{timestamp}\"\n",
    "        \n",
    "        print(f\"Model trained with accuracy: {self.accuracy:.4f}\")\n",
    "        return {\n",
    "            'accuracy': self.accuracy,\n",
    "            'report': report,\n",
    "            'classes': self.classes,\n",
    "            'model_version': self.model_version,\n",
    "            'training_data_size': self.training_data_size\n",
    "        }\n",
    "    \n",
    "    def create_ensemble(self, uploaded_models, base_weight=BASE_MODEL_WEIGHT):\n",
    "        if not self.is_trained or not uploaded_models:\n",
    "            return False\n",
    "        \n",
    "        # Initialize ensemble with base model\n",
    "        estimators = [('base', self.model)]\n",
    "        self.component_models = {'base': 'Base model'}\n",
    "        \n",
    "        # Add placeholder models for each user model\n",
    "        for idx, model in enumerate(uploaded_models):\n",
    "            model_id = model.get('id', f'user{idx}')\n",
    "            \n",
    "            # Create compatible model\n",
    "            n_features = self.vectorizer.get_feature_names_out().shape[0]\n",
    "            user_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            \n",
    "            # Fit with dummy data\n",
    "            X_dummy = np.random.random((10, n_features))\n",
    "            y_dummy = np.random.choice(self.classes, 10)\n",
    "            user_model.fit(X_dummy, y_dummy)\n",
    "            \n",
    "            # Add to ensemble\n",
    "            estimator_name = f'user{idx}'\n",
    "            estimators.append((estimator_name, user_model))\n",
    "            self.component_models[estimator_name] = {\n",
    "                'id': model_id,\n",
    "                'device_id': model.get('device_id', 'unknown')\n",
    "            }\n",
    "        \n",
    "        # Create VotingClassifier\n",
    "        weights = [base_weight] + [USER_MODEL_WEIGHT] * len(uploaded_models)\n",
    "        self.model = VotingClassifier(estimators=estimators, voting='soft', weights=weights)\n",
    "        self.is_ensemble = True\n",
    "        return True\n",
    "    \n",
    "    def _convert_to_coreml(self, output_path):\n",
    "        # Define prediction function\n",
    "        def predict_intent(text):\n",
    "            processed_text = preprocess_text(text)\n",
    "            vec_text = self.vectorizer.transform([processed_text])\n",
    "            intent = self.model.predict(vec_text)[0]\n",
    "            probabilities = self.model.predict_proba(vec_text)[0]\n",
    "            return intent, probabilities\n",
    "        \n",
    "        # Apply version compatibility fixes to coremltools if needed\n",
    "        if 'coremltools' in sys.modules:\n",
    "            import coremltools as ct\n",
    "            # Monkey patch version checks if needed\n",
    "            if hasattr(ct, '_dependency_check'):\n",
    "                for check_name in ['verify_scikit_learn_version', 'verify_tensorflow_version', \n",
    "                                'verify_torch_version', 'verify_xgboost_version']:\n",
    "                    if hasattr(ct._dependency_check, check_name):\n",
    "                        setattr(ct._dependency_check, check_name, lambda *args, **kwargs: True)\n",
    "        \n",
    "        # Handle missing libcoremlpython module\n",
    "        has_error = False\n",
    "        try:\n",
    "            # Try to import librarypy to check its availability\n",
    "            import coremltools.libcoremlpython\n",
    "        except ImportError:\n",
    "            has_error = True\n",
    "            print(\"Warning: Missing libcoremlpython - using simplified conversion\")\n",
    "        \n",
    "        # Use try-except to handle potential conversion errors\n",
    "        try:\n",
    "            # Standard conversion with complete outputs\n",
    "            coreml_model = ct.convert(\n",
    "                predict_intent,\n",
    "                inputs=[ct.TensorType(shape=(1,), dtype=str)],\n",
    "                outputs=[\n",
    "                    ct.TensorType(name='intent'),\n",
    "                    ct.TensorType(name='probabilities', dtype=np.float32)\n",
    "                ],\n",
    "                classifier_config=ct.ClassifierConfig(self.classes)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Standard conversion failed: {e}\")\n",
    "            # Try alternative simplified conversion\n",
    "            try:\n",
    "                coreml_model = ct.convert(\n",
    "                    predict_intent,\n",
    "                    inputs=[ct.TensorType(shape=(1,), dtype=str)],\n",
    "                    outputs=[ct.TensorType(name='intent')]\n",
    "                )\n",
    "                print(\"Used simplified conversion due to errors\")\n",
    "            except Exception as e2:\n",
    "                print(f\"All conversion attempts failed: {e2}\")\n",
    "                return False\n",
    "        \n",
    "        # Add metadata\n",
    "        coreml_model.user_defined_metadata['version'] = self.model_version\n",
    "        coreml_model.user_defined_metadata['training_date'] = self.training_date\n",
    "        coreml_model.user_defined_metadata['accuracy'] = str(self.accuracy)\n",
    "        \n",
    "        # Save model\n",
    "        coreml_model.save(output_path)\n",
    "        return True\n",
    "        def predict_intent(text):\n",
    "            processed_text = preprocess_text(text)\n",
    "            vec_text = self.vectorizer.transform([processed_text])\n",
    "            intent = self.model.predict(vec_text)[0]\n",
    "            probabilities = self.model.predict_proba(vec_text)[0]\n",
    "            return intent, probabilities\n",
    "        \n",
    "        # Convert to CoreML\n",
    "        coreml_model = ct.convert(\n",
    "            predict_intent,\n",
    "            inputs=[ct.TensorType(shape=(1,), dtype=str)],\n",
    "            outputs=[\n",
    "                ct.TensorType(name='intent'),\n",
    "                ct.TensorType(name='probabilities', dtype=np.float32)\n",
    "            ],\n",
    "            classifier_config=ct.ClassifierConfig(self.classes)\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        coreml_model.user_defined_metadata['version'] = self.model_version\n",
    "        coreml_model.user_defined_metadata['training_date'] = self.training_date\n",
    "        coreml_model.user_defined_metadata['accuracy'] = str(self.accuracy)\n",
    "        \n",
    "        # Save model\n",
    "        coreml_model.save(output_path)\n",
    "        return True\n",
    "    \n",
    "    def save(self, output_dir=None):\n",
    "        # Create temp dir if not provided\n",
    "        if output_dir is None:\n",
    "            output_dir = tempfile.mkdtemp()\n",
    "            \n",
    "        # Define paths\n",
    "        sklearn_path = os.path.join(output_dir, f\"intent_classifier_{self.model_version}.joblib\")\n",
    "        info_path = os.path.join(output_dir, f\"model_info_{self.model_version}.json\")\n",
    "        coreml_path = os.path.join(output_dir, f\"model_{self.model_version}.mlmodel\")\n",
    "        \n",
    "        # Save sklearn model\n",
    "        joblib.dump((self.vectorizer, self.model), sklearn_path)\n",
    "        \n",
    "        # Create and save CoreML model\n",
    "        self._convert_to_coreml(coreml_path)\n",
    "        \n",
    "        # Save model info\n",
    "        model_info = {\n",
    "            'version': self.model_version,\n",
    "            'accuracy': self.accuracy,\n",
    "            'training_data_size': self.training_data_size,\n",
    "            'training_date': self.training_date,\n",
    "            'is_ensemble': self.is_ensemble,\n",
    "            'component_models': len(self.component_models) if self.component_models else 1,\n",
    "            'classes': self.classes\n",
    "        }\n",
    "        \n",
    "        with open(info_path, 'w') as f:\n",
    "            json.dump(model_info, f)\n",
    "        \n",
    "        return {\n",
    "            'sklearn_path': sklearn_path,\n",
    "            'coreml_path': coreml_path,\n",
    "            'info_path': info_path,\n",
    "            'model_info': model_info\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}